from transformers import AutoTokenizer, AutoModel
import tiktoken

tokenizer_name = "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
# tokenizer_name = "jinaai/jina-embeddings-v2-base-en"
# tokenizer_name = "mymusise/CPM-GPT2"
# tokenizer_name = "gpt2"
# tokenizer_name = "bert-base-chinese"
# tokenizer_name = "BAAI/llm-embedder"
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

inps = [
        "Hell12o,, world! complexness123", 
        "ï¼Œå¤§12å®¶å¥½gptï¼Œæˆ‘æ˜¯GPTã€‚ä½ å¥½ã€‚ä¸­åœ‹é¾", 
        "ä½ å¥½ï¼Œä¸–ç•Œï¼", 
        "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼", 
        "syÃ¶mme \t  tÃ¤Ã¤llÃ¤ tÃ¤nÃ¤Ã¤n",
        "ğŸ™‚ğŸ™‚ğŸ™‚ğŸ˜’ğŸ˜’ğŸ˜’ğŸğŸğŸ‘ğŸ˜—âšœï¸ğŸ••â›„â˜ƒï¸",
        "1231 2431431",
    ]

print("Using tokenizer:", tokenizer_name)
for inp in inps:
    oup = tokenizer(inp, return_tensors="pt").input_ids[0].tolist()
    print(f"{oup} is {tokenizer.decode(oup)}")
    for token in oup:
        print(f"{token} <--> {tokenizer.decode([token])}")
    print("\n\n")
    # print(f"{oup} is {tokenizer.decode(oup)}")

    # print(f"{inp} is tokenized as {oup}")
    # for token in oup:
        # print(f"{token} is {tokenizer.decode([token])}")

